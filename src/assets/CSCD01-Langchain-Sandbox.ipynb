{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2375891",
   "metadata": {},
   "source": [
    "# Langchain!\n",
    "\n",
    "## What is this?\n",
    "\n",
    "The goal of this lab to introduce you to working with [Langchain](https://github.com/langchain-ai/langchain) for Python. [Langchain.js](https://github.com/langchain-ai/langchainjs) is also available for JavaScript/TypeScript, and (is supposed to be) very similar to the Python version. We don't need you to know every detail of the library (it's pretty huge), but you will need to study and understand the part(s) related to your eventual open source contribution. This notebook, alongside [the official documentation](https://python.langchain.com/v0.2/docs/how_to/), should give you a high-level overview.\n",
    "\n",
    "## Install Langchain packages\n",
    "\n",
    "In your Python environment, install `langchain-core`, `langchain` and `langchain-community`. The contents of each package are [briefly described here](https://python.langchain.com/v0.2/docs/concepts/#architecture), but in short, `langchain-community` has most of the third-party integrations—which is where you may want to consider contributing :)\n",
    "\n",
    "Oh, and we'll need `requests` (just an HTTP client).\n",
    "\n",
    "Run the next cell to install all that: <small><details><summary>(why v0.2 and not v0.3?)</summary>\n",
    "<ul>\n",
    "<li>v0.3 was released as I was writing this lab</li>\n",
    "<li>the examples I'm linking to in the official documentation haven't been updated to work with v0.3</li>\n",
    "</details></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef1af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'langchain-core>=0.2,<0.3' 'langchain>=0.2,<0.3' 'langchain-community>=0.2,<0.3' requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb68ee10",
   "metadata": {},
   "source": [
    "## Finding a language model\n",
    "\n",
    "Langchain supports two kinds of models: [\"chat\" models](https://python.langchain.com/v0.2/docs/concepts/#chat-models), which are fine-tuned for message-based conversations, and [\"LLM\" models](), which just take text (\"once upon a time…\") and spit out a completion (\"…there was a\"). Normally, you'd have to bring your own model to do anything with Langchain. But the author of this notebook already paid $5 to OpenAI during last year's D01. Might as well put it to use…\n",
    "\n",
    "Run the next cell, and provide your UofT email + student number. This gives you access to a chat model you can experiment with. Feel free to mess around with this! But please do **not** spam or abuse this model, lest it burn a hole in your TA's pocket. (we will know who you are…)\n",
    "\n",
    "If you want to run a serious Langchain application, or try some of the more advanced features like real-time streaming, use one of the [official chat models](https://python.langchain.com/v0.2/docs/integrations/chat/). These will require either a paid API key (if you use someone else's GPUs) or something like [Ollama](https://python.langchain.com/v0.2/docs/integrations/chat/ollama/) (running on your own GPUs). (This space moves so fast that I'm not sure if Ollama is still the \"new hotness,\" but it works.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3acf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "API_TOKEN = re.match('[^@]+(?=@)', input(\"Enter your UofT email: \")).group(0) + '-' + input(\"Enter your student number: \")\n",
    "print(f'API token: {API_TOKEN}')\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Sequence, Type, Union\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.language_models import BaseChatModel, LanguageModelInput\n",
    "from langchain_core.load import dumps, loads\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "import requests\n",
    "\n",
    "\n",
    "def fetch_from_relay(token, task, payload):\n",
    "    res = requests.post('https://us-east5-cscd01-435202.cloudfunctions.net/chatmodel-relay', params={\n",
    "        'token': token,\n",
    "    }, json={\n",
    "        'task': task,\n",
    "        'payload': payload\n",
    "    })\n",
    "\n",
    "    if res.status_code != 200:\n",
    "        headers = \"\\n\".join((f\"{k}: {v}\" for k, v in res.headers.items()))\n",
    "        raise Exception(f\"Relay returned an error:\\nHTTP {res.status_code}\\n{headers}\\n\\n{res.text}\")\n",
    "\n",
    "    return loads(res.text)\n",
    "\n",
    "class ChatCSCD01(BaseChatModel):\n",
    "    \"\"\"Langchain chat model for use in CSCD01.\n",
    "\n",
    "    This is a network service for your personal use as a student.\n",
    "    You must authenticate with the credentials provided by your TA.\n",
    "\n",
    "    Feel free to use this service for testing.\n",
    "    But do NOT spam or abuse it; we will know who you are.\n",
    "\n",
    "    If you want to run a serious Langchain application, use one of the\n",
    "    [official chat models](https://python.langchain.com/v0.2/docs/integrations/chat/).\n",
    "\n",
    "    No need to understand this class, but for details on how it works, see\n",
    "    - https://python.langchain.com/v0.2/docs/how_to/custom_chat_model/\n",
    "    - https://python.langchain.com/v0.2/docs/how_to/serialization/\n",
    "    \"\"\"\n",
    "\n",
    "    token: str\n",
    "    \"\"\"Authentication token.\"\"\"\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        message_result = fetch_from_relay(self.token, 'chat', {\n",
    "            'messages': [dumps(m) for m in messages],\n",
    "            'stop': stop,\n",
    "            'kwargs': kwargs\n",
    "        })\n",
    "        return ChatResult(generations=[ChatGeneration(message=message_result)])\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"chat-cscd01\"\n",
    "\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]],\n",
    "        *,\n",
    "        strict: Optional[bool] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, BaseMessage]:\n",
    "        return super().bind(tools=[convert_to_openai_tool(tool, strict=strict) for tool in tools], **kwargs)\n",
    "\n",
    "class CSCD01Embeddings(Embeddings):\n",
    "    \"\"\"Langchain embedding model for use in CSCD01.\n",
    "\n",
    "    This is a network service for your personal use as a student.\n",
    "    You must authenticate with the credentials provided by your TA.\n",
    "\n",
    "    Feel free to use this service for testing.\n",
    "    But do NOT spam or abuse it; we will know who you are.\n",
    "\n",
    "    If you want to run a serious Langchain application, use one of the\n",
    "    [official embedding models](https://python.langchain.com/v0.2/docs/integrations/text_embedding/).\n",
    "    \"\"\"\n",
    "\n",
    "    token: str\n",
    "    \"\"\"Authentication token.\"\"\"\n",
    "\n",
    "    def __init__(self, token: str):\n",
    "        self.token = token\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return fetch_from_relay(self.token, 'embed_documents', texts)\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return fetch_from_relay(self.token, 'embed_query', text)\n",
    "\n",
    "\n",
    "model = ChatCSCD01(token=API_TOKEN)\n",
    "embeddings = CSCD01Embeddings(token=API_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b70442",
   "metadata": {},
   "source": [
    "## Using the model\n",
    "\n",
    "To use a chat model, it's as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eda8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke('a man walked into a bar. (I need a punchline)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911acfe4",
   "metadata": {},
   "source": [
    "But `.invoke` is not `ChatModel`-specific; it's part of the generic [`Runnable` interface](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface). This is where the \"chain\" part of \"Langchain\" comes in.\n",
    "\n",
    "## Chaining, prompts, and output parsers\n",
    "\n",
    "Run this next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed1b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template('a {character} walked into a bar. (I need a punchline)')\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f593982",
   "metadata": {},
   "source": [
    "There are five `Runnable`s here: `prompt`, `model`, `StrOutputParser()`, `prompt | model` and `(prompt | model) | StrOutputParser()`, the last of which we called `chain`. The `|` (\"pipe\") operator behaves as you would expect: it streams output to input, left to right.\n",
    "\n",
    "`.invoke` on a chain instance thus feeds the first `Runnable` and produces the result of the last `Runnable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e3404",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({'character': input('Who walked into a bar? ')})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cccbe4f",
   "metadata": {},
   "source": [
    "Different `Runnable`s have different (heterogeneous) input and output. [`PromptTemplate`s](https://python.langchain.com/v0.2/docs/concepts/#prompt-templates) take a dictionary of values and produce a prompt message that a `ChatModel` can consume; [output parsers](https://python.langchain.com/v0.2/docs/concepts/#output-parsers) (attempt to) make structure from the chaos that is language model output.\n",
    "\n",
    "Python functions can also be treated as `Runnable`s in a chain, as can other chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133679af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def show(output):\n",
    "  print('show:', output)\n",
    "  return output\n",
    "\n",
    "extract_code = (\n",
    "      PromptTemplate.from_template('extract just the code (nothing else!) from the following:\\n{code}')\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: {'code': x})\n",
    ")\n",
    "\n",
    "(\n",
    "      PromptTemplate.from_template('write a Python program that prints the number of days left in the year')\n",
    "    | show\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    "\n",
    "    | (lambda x: {'code': x})\n",
    "    | show\n",
    "    | extract_code\n",
    "    | show\n",
    "\n",
    "    | PromptTemplate.from_template('what does this program do?\\n{code}')\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ").invoke({'date': datetime.today().strftime('%Y-%m-%d')})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac5b191",
   "metadata": {},
   "source": [
    "### Try it: Dummy data generation\n",
    "\n",
    "By adapting [this example](https://python.langchain.com/v0.2/docs/how_to/output_parser_json/), use our chat model to generate place listings: name, rating, lat/lon Google Maps link, and anything else you like.\n",
    "\n",
    "e.g. (Python output):\n",
    ">     {'name': 'The Enchanted Coffeehouse',\n",
    ">      'rating': 4.7,\n",
    ">      'maps_link': 'https://www.google.com/maps/place/40.712776,-74.005974',\n",
    ">      ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Define your desired data structure.\n",
    "# TODO\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "# Tip: you may have to coax the agent into being a bit creative.\n",
    "# TODO\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "# TODO\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  # TODO\n",
    ")\n",
    "# If you're curious...\n",
    "print(\"Format instructions:\", parser.get_format_instructions())\n",
    "\n",
    "chain = ... # TODO\n",
    "\n",
    "# Generate some place listings\n",
    "places = [chain.invoke(\n",
    "  # TODO\n",
    ") for _ in range(6)]\n",
    "places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24220c2",
   "metadata": {},
   "source": [
    "## Memory: document loaders, vector stores, and retrievers\n",
    "\n",
    "Modern language models work by \"[embedding](https://duckduckgo.com/?q=language+model+embedding)\" text into a (very) high-dimensional vector space. Such a representation [efficiently encodes semantic similarity between tokens](https://towardsdatascience.com/text-embeddings-comprehensive-guide-afd97fce8fb5). \"Vector stores\" or \"vector databases\" are designed for efficient embedding storage and search.\n",
    "\n",
    "Why should you care about this implementation detail? Here's one reason: say you have a large dataset you want your language model to reason with. At first, you might try to stuff all of it into the prompt string. Unfortunately, transformer models (like OpenAI's) have a \"limited context window.\" In a way analogous to the limits of your own short-term memory, the model will tend to \"forget\" things that you told it a long time ago and never repeated. But, how do you, as a human, manage to reason with more information than you can hold in your short-term memory? You entrust it to your long-term memory (or your notebook, or the internet…). Vector stores perform a similar function for language models: at the model's request, you can retrieve information from a vector store, effectively augmenting its knowledge. This pattern is known as [\"retrieval-augmented generation\" (RAG)](https://towardsdatascience.com/rag-how-to-talk-to-your-data-eaf5469b83b0). (btw, don't take the human memory anaology too seriously.)\n",
    "\n",
    "Chroma is a popular vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd4c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0de007",
   "metadata": {},
   "source": [
    "\"Document loaders\" are convenience utilities that break data down into individual documents that a model can ask to \"see.\" There are [many built-in loaders](https://python.langchain.com/v0.2/docs/integrations/document_loaders/) that integrate with various data sources.\n",
    "\n",
    "[\"Retrievers\"](https://python.langchain.com/v0.2/docs/integrations/retrievers/) are `Runnable`s that query a data store. Vector stores are trivially retrievers ([`.as_retriever`](https://python.langchain.com/v0.2/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html#langchain_core.vectorstores.base.VectorStore.as_retriever))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c5b40",
   "metadata": {},
   "source": [
    "### Try it: fake RAG\n",
    "\n",
    "Read [this tutorial](https://python.langchain.com/v0.2/docs/tutorials/retrievers/) and/or [these](https://python.langchain.com/v0.2/docs/how_to/vectorstores/) [pages](https://python.langchain.com/v0.2/docs/how_to/vectorstore_retriever/). (You don't need to sign up for LangSmith or use any third-party API, unless you want to.) Then, by adapting [this example](https://python.langchain.com/v0.2/docs/tutorials/rag/), create an agent that can answer questions about the place listings you generated above.\n",
    "\n",
    "This is \"fake\" RAG because the documents are just dumped into the prompt context. We'll fix that soon…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a731015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from langchain import hub\n",
    "from langchain_core.document_loaders.base import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "# TODO: Complete this document loader\n",
    "class PlacesDocumentLoader(BaseLoader):\n",
    "    \"\"\"Simple document loader that yields the places generated above.\n",
    "\n",
    "    See:\n",
    "    - [BaseLoader](https://python.langchain.com/v0.2/api_reference/core/document_loaders/langchain_core.document_loaders.base.BaseLoader.html)\n",
    "    - [lazy_load](https://python.langchain.com/v0.2/api_reference/_modules/langchain_core/document_loaders/base.html#BaseLoader.lazy_load)\n",
    "    - [Generators in Python](https://realpython.com/introduction-to-python-generators/)\n",
    "    - [Document](https://python.langchain.com/v0.2/api_reference/core/documents/langchain_core.documents.base.Document.html#document)\n",
    "    \"\"\"\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        pass # TODO\n",
    "\n",
    "\n",
    "# Initialize your document loader, and load!\n",
    "# For this example, we'll just get all documents at once.\n",
    "loader = ... # TODO\n",
    "docs = ... # TODO\n",
    "\n",
    "# Split the documents into bite-sized chunks (that an LLM can handle).\n",
    "splits = ... # TODO\n",
    "\n",
    "# Dump the split documents into a vector store.\n",
    "vectorstore = ... # TODO (hint: use `embeddings`, defined at the top of the notebook, as your embedding model)\n",
    "\n",
    "# Retrieve and generate.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"can you give me a Google Maps link for a place to go for lunch?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aecf8b",
   "metadata": {},
   "source": [
    "## Tools and agents\n",
    "\n",
    "The final pieces of the puzzle for true RAG are [tools](https://python.langchain.com/v0.2/docs/concepts/#tools). In short, a tool is something a language model can \"ask to use\" with input of its choice. Modern ChatGPT has tools like web search and a Python runner (try it out!). In Langchain, tools are (you guessed it) `Runnable`s; but they also carry information that informs the model what the tool is and how to call it. Plain functions can be [wrapped with `@tool`](https://python.langchain.com/v0.2/api_reference/core/tools/langchain_core.tools.convert.tool.html#tool) to automatically generate this information.\n",
    "\n",
    "When a model can take an action (such as using a tool), it's called an [agent](https://python.langchain.com/v0.2/docs/concepts/#agents). A common pattern for making agents with LLMs is [reason + act (ReAct)](https://python.langchain.com/v0.2/docs/concepts/#react-agents). Of course, to allow a model to take an external action and await feedback, we have be able to call both the tools and the model in a (possibly endless) loop; a simple pipeline won't suffice. We could certainly implement this ourselves, but no need: the Langchain team made another library just for this.\n",
    "\n",
    "### LangGraph\n",
    "\n",
    "LangGraph lets you build state machines for LLM-based agent control, with out-of-the-box support for the ReAct pattern. Notably, chat history is preserved between state transitions, and you can treat any Langchain tool as a state in the graph.\n",
    "\n",
    "If you need a refresher on what \"state machine\" means, think back to \"FSMs\" from B58 or \"PDAs\" from B36, but where each state can access auxiliary memory. (If that sounds like a Turing machine—it is.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73474c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langgraph typing_extensions==4.12.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e8e2a",
   "metadata": {},
   "source": [
    "### Try it: true RAG\n",
    "\n",
    "Follow [this example](https://langchain-ai.github.io/langgraph/#example) to\n",
    "- define a callable tool;\n",
    "- define states and persistent memory for your agent (state machine);\n",
    "- run your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7799e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "# Define the tool for the agent to use\n",
    "# TODO\n",
    "# (hint: use your Chroma retriever)\n",
    "# (hint: either use @tool with a good docstring, or follow [this guide](https://python.langchain.com/v0.2/docs/how_to/convert_runnable_to_tool/)\n",
    "#        and pass a good description, so that the model knows what's up)\n",
    "\n",
    "\n",
    "tools = # TODO\n",
    "tool_node = (ToolNode(tools) | show)\n",
    "model = model.bind_tools(tools)\n",
    "\n",
    "# Transition function that runs after the model has returned a response\n",
    "def after_agent_transition(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    # TODO (you can essentially copy the example code...)\n",
    "    pass\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    print('messages:', messages)\n",
    "\n",
    "    response = model.invoke(messages)\n",
    "    print('response:', response)\n",
    "\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "# TODO\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "# TODO\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node.\n",
    "    # TODO: your start state\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    # TODO: your transition function\n",
    ")\n",
    "\n",
    "# We now add a normal edge from TODO to TODO.\n",
    "# This means that after TODO is called, TODO node is called next.\n",
    "workflow.add_edge(...)\n",
    "\n",
    "# Initialize memory to persist state between graph runs\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable.\n",
    "# Note that we're (optionally) passing the memory when compiling the graph\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Use the Runnable\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"please search for a place I can go for lunch. I cannot answer any further questions.\")]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "final_state[\"messages\"][-1].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
