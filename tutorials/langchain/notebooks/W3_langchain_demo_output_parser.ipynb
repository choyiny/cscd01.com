{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45cf8c54-98e8-4df6-add0-681fc7c39b22",
   "metadata": {},
   "source": [
    "## Langchain demo of output parser\n",
    "\n",
    "This exercise will cover output parsers, what they are, why we use them and some examples.\n",
    "First, let's get all the config and imports stuff in order.\n",
    "\n",
    "1. Notice the the below code block has `import config`, find the `config.py` file (in the same directory) and put your openai API key in here.  If you do not have one yet, create a new account and refer to [here](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key) to find your openai API key. \n",
    "\n",
    "2. Once you have your key, update the key in `config.py`, it should have the format of `sk-XXXXXXXXXXXXXXXXXX` (number of X's is not exact so don't take that literally please).\n",
    "\n",
    "3. Run the block below once the above 2 are done so setup the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfa9eccd-7af7-4a2c-90b5-a7540a601d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "import config\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = config.OPENAI_API_KEY\n",
    "\n",
    "# uncomment below if you want to use huggingface instead, when you run this, you will see an input box below, paste in your huggingface token in there\n",
    "# and wait to see \"........\"\n",
    "from getpass import getpass\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb71929-3551-4e08-b992-2e1ef0baed5f",
   "metadata": {},
   "source": [
    "### What are output parsers?\n",
    "We have seen that language models output text, but usually you'd like to get more structured information output than just text.  \n",
    "\n",
    "Let's take an example:\n",
    "Suppose you ask your llm to build you a car from a description, the output could be a long string such as `2020 Honda CR-V AWD (All Wheel Drive) ...`.  This is great, but how can we actually use this information?\n",
    "\n",
    "Thinking from an OOP perspective, it would be better for us to ***model*** this output in an object so that it can interact with other stuff.  In other words, we would like some struct/class/object that might look something like this:\n",
    "\n",
    "```\n",
    "class Car:\n",
    "    model\n",
    "    make\n",
    "    drivetrain (AWD, 4WD, RWD, ...)\n",
    "    ...\n",
    "```\n",
    "\n",
    "Output parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\n",
    "\n",
    "\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
    "And then one optional one:\n",
    "\n",
    "\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt t\n",
    "\n",
    "That's a lot of text to read, let's illustrate with some more examples with the main output parser, the `PydanticOutpu.o do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b185a4a-9080-434c-96a8-7b7d6681269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model with temperature\n",
    "# temperature is a measure of how creative we want the model to be\n",
    "model_name = 'text-davinci-003'\n",
    "temperature = 0.5\n",
    "model = OpenAI(model_name=model_name, temperature=temperature)\n",
    "\n",
    "# uncomment below to test you have credits, if not go back up and use huggingface\n",
    "# name = model(\"say hi\")\n",
    "\n",
    "# uncomment below to use huggingface\n",
    "repo_id = \"google/flan-t5-xxl\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "hmodel = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": temperature, \"max_length\": 64}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d8a0749-ebe1-4ec6-bff9-866660b1aed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name=\"Spicy Tomato Soup\" ingredients=\"tomatoes, onion, garlic\n"
     ]
    }
   ],
   "source": [
    "# Define your desired data structure.\n",
    "class Dish(BaseModel):\n",
    "    name: str = Field(description=\"name of the dish\")\n",
    "    ingredients: List[str] = Field(description=\"list of ingredients required\")\n",
    "\n",
    "# Now to setup the use of the model\n",
    "query = \"Generate a recipe name, and list the ingredients required for the recipe\"\n",
    "\n",
    "# Notice we set the object to our Dish class\n",
    "parser = PydanticOutputParser(pydantic_object=Dish)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "\n",
    "_input = prompt.format_prompt(query=query)\n",
    "inputstr = _input.to_string()\n",
    "\n",
    "# to use openai\n",
    "output = model(inputstr)\n",
    "parser.parse(output)\n",
    "\n",
    "# uncomment below (and comment above block 2 lines) to use huggingface\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=hmodel)\n",
    "# output = llm_chain.run(inputstr)\n",
    "# print(output)\n",
    "# parser.parse(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050ffec-3725-4660-9887-c2eb7fccd2f6",
   "metadata": {},
   "source": [
    "Below, make another Python class called ChickenJoke with the following properties:\n",
    "setup: string -> the setup for the joke (Why did the chicken cross the road)\n",
    "answer: string -> the resolution for the joke\n",
    "\n",
    "Ask your llm to come up with a funny answer, and parse it into the ChickenJoke class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ad221-7fb3-477a-9869-d5a0d65e06ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
